{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Implement a Neural Network for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will implement a neural network that performs sentiment analysis for a binary classification problem. You will:\n",
    "\n",
    "1. Load the book review data set.\n",
    "2. Create training and test datasets.\n",
    "3. Transform the training and test text data using a TF-IDF vectorizer. \n",
    "4. Construct a neural network\n",
    "5. Train the neural network.\n",
    "6. Compare the model's performance on the training data vs test data.\n",
    "7. Improve its generalization performance.\n",
    "\n",
    "For this lab, you may use the following demos as reference: <i>Transforming Text into Features for Sentiment Analysis</i> and <i>Implementing a Neural Network in Keras</i>. \n",
    "\n",
    "**<font color='red'>Note: some of the code cells in this notebook may take a while to run</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:  Load the Data Set\n",
    "\n",
    "We will work with the book review data set that contains book reviews taken from Amazon.com reviews.\n",
    "\n",
    "<b>Task</b>: In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`.\n",
    "\n",
    "You will be working with the file named \"bookReviews.csv\" that is located in a folder named \"data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/bookReviews.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was perhaps the best of Johannes Steinhof...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This very fascinating book is a story written ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The four tales in this collection are beautifu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The book contained more profanity than I expec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have now entered a second time of deep conc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Positive Review\n",
       "0  This was perhaps the best of Johannes Steinhof...             True\n",
       "1  This very fascinating book is a story written ...             True\n",
       "2  The four tales in this collection are beautifu...             True\n",
       "3  The book contained more profanity than I expec...            False\n",
       "4  We have now entered a second time of deep conc...             True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Review', 'Positive Review'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df.shape\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Labeled Examples\n",
    "\n",
    "<b>Task</b>: Create labeled examples from DataFrame `df`. \n",
    "In the code cell below carry out the following steps:\n",
    "\n",
    "* Get the `Positive_Review` column from DataFrame `df` and assign it to the variable `y`. This will be our label.\n",
    "* Get the `Review` column from  DataFrame `df` and assign it to the variable `X`. This will be our feature. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get the 'Positive Review' column as the label 'y'\n",
    "y = df['Positive Review']\n",
    "\n",
    "# Step 2: Get the 'Review' column as the feature 'X'\n",
    "X = df['Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This was perhaps the best of Johannes Steinhof...\n",
       "1    This very fascinating book is a story written ...\n",
       "2    The four tales in this collection are beautifu...\n",
       "3    The book contained more profanity than I expec...\n",
       "4    We have now entered a second time of deep conc...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1973,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Labeled Examples into Training and Test Sets\n",
    "\n",
    "\n",
    "<b>Task</b>: In the code cell below, create training and test sets out of the labeled examples. \n",
    "\n",
    "1. Use scikit-learn's `train_test_split()` function to create the data sets.\n",
    "\n",
    "2. Specify:\n",
    "    * A test set that is 20 percent of the size of the data set.\n",
    "    * A seed value of '1234'. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1369    As my brother said when flipping through this ...\n",
       "1366    Cooper's book is yet another warm and fuzzy ma...\n",
       "385     I have many robot books and this is the best a...\n",
       "750     As China re-emerges as a dominant power in the...\n",
       "643     I have been a huge fan of Michael Crichton for...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:  Implement TF-IDF Vectorizer to Transform Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code cell below, you will transform the features into numerical vectors using `TfidfVectorizer`. \n",
    "\n",
    "\n",
    "<b>Task:</b> Follow the steps to complete the code in the cell below:\n",
    "\n",
    "1. Create a `TfidfVectorizer` object and save it to the variable `tfidf_vectorizer`.\n",
    "\n",
    "2. Call `tfidf_vectorizer.fit()` to fit the vectorizer to the training data `X_train`.\n",
    "\n",
    "3. Call the `tfidf_vectorizer.transform()` method to use the fitted vectorizer to transform the training data `X_train`. Save the result to `X_train_tfidf`.\n",
    "\n",
    "4. Call the `tfidf_vectorizer.transform()` method to use the fitted vectorizer to transform the test data `X_test`. Save the result to `X_test_tfidf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a TfidfVectorizer object \n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# 2. Fit the vectorizer to X_train\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "\n",
    "# 3. Using the fitted vectorizer, transform the training data \n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "\n",
    "\n",
    "# 4. Using the fitted vectorizer, transform the test data \n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When constructing our neural network, we will have to specify the `input_shape`, meaning the dimensionality of the input layer. This corresponds to the dimension of each of the training examples, which in our case is our vocabulary size. Run the code cell below to see the vocabulary size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19029\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Construct a Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.  Define Model Structure\n",
    "\n",
    "Next we will create our neural network structure. We will create an input layer, three hidden layers and an output layer:\n",
    "\n",
    "* <b>Input layer</b>: The input layer will have the input shape corresponding to the vocabulary size. \n",
    "* <b>Hidden layers</b>: We will create three hidden layers of widths (number of nodes) 64, 32, and 16. They will utilize the ReLu activation function. \n",
    "* <b>Output layer</b>: The output layer will have a width of 1. The output layer will utilize the sigmoid activation function. Since we are working with binary classification, we will be using the sigmoid activation function to map the output to a probability between 0.0 and 1.0. We can later set a threshold and assume that the prediction is class 1 if the probability is larger than or equal to our threshold, or class 0 if it is lower than our threshold.\n",
    "\n",
    "To construct the neural network model using Keras, we will do the following:\n",
    "* We will use the Keras `Sequential` class to group a stack of layers. This will be our neural network model object.\n",
    "* We will use the `Dense` class to create each layer. \n",
    "* We will add each layer to the neural network model object.\n",
    "\n",
    "\n",
    "<b>Task:</b> Follow these steps to complete the code in the cell below:\n",
    "\n",
    "1. Create the neural network model object. \n",
    "    * Use ``keras.Sequential() `` to create a model object, and assign the result to the variable ```nn_model```.\n",
    "    \n",
    "    \n",
    "2. Create the input layer: \n",
    "    * Call `keras.layers.Dense()` with the argument `input_shape=(vocabulary_size,)` to specify the dimension of the input.\n",
    "    * Assign the results to the variable `input_layer`.\n",
    "    * Use `nn_model.add(input_layer)` to add the layer `input_layer` to the neural network model object.\n",
    "\n",
    "\n",
    "3. Create the first hidden layer:\n",
    "    * Call `keras.layers.Dense()` with the arguments `units=64` and `activation='relu'`. \n",
    "    * Assign the results to the variable `hidden_layer_1`.\n",
    "    * Use `nn_model.add(hidden_layer_1)` to add the layer `hidden_layer_1` to the neural network model object.\n",
    "\n",
    "\n",
    "4. Create the second hidden layer using the same approach that you used to create the first hidden layer, specifying 32 units and the `relu` activation function. \n",
    "    * Assign the results to the variable `hidden_layer_2`.\n",
    "    * Add the layer to the neural network model object.\n",
    "    \n",
    "    \n",
    "5. Create the third hidden layer using the same approach that you used to create the first two hidden layers, specifying 16 units and the `relu` activation function. \n",
    "    * Assign the results to the variable `hidden_layer_3`.\n",
    "    * Add the layer to the neural network model object.\n",
    "\n",
    "\n",
    "6. Create the output layer using the same approach that you used to create the hidden layers, specifying 1 unit and the `signmoid` activation function. \n",
    "   * Assign the results to the variable `output_layer`.\n",
    "   * Add the layer to the neural network model object.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 64)                1217920   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,224,705\n",
      "Trainable params: 1,224,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "# 1. Create model object\n",
    "nn_model = Sequential()\n",
    "\n",
    "# 2. Create the input layer and add it to the model object: \n",
    "\n",
    "# Create input layer:\n",
    "input_layer =Dense(units=64, activation='relu', input_shape=(vocabulary_size,))\n",
    "\n",
    "# Add input_layer to the model object:\n",
    "nn_model.add(input_layer)\n",
    "\n",
    "# 3. Create the first hidden layer and add it to the model object:\n",
    "\n",
    "# Create input layer:\n",
    "hidden_layer_1 = Dense(units=64, activation='relu')\n",
    "\n",
    "\n",
    "# Add hidden_layer_1 to the model object:\n",
    "nn_model.add(hidden_layer_1)\n",
    "\n",
    "##NEW \n",
    "nn_model.add(Dropout(0.25))\n",
    "\n",
    "# 4. Create the second layer and add it to the model object:\n",
    "\n",
    "# Create input layer:\n",
    "hidden_layer_2 = Dense(units=32, activation='relu')\n",
    "\n",
    "# Add hidden_layer_2 to the model object:\n",
    "nn_model.add(hidden_layer_2)\n",
    "\n",
    "#NEW\n",
    "nn_model.add(Dropout(0.25))\n",
    "\n",
    "# 5. Create the third layer and add it to the model object:\n",
    "\n",
    "# Create input layer:\n",
    "hidden_layer_3 = Dense(units=16, activation='relu')\n",
    "\n",
    "# Add hidden_layer_3 to the model object:\n",
    "nn_model.add(hidden_layer_3)\n",
    "\n",
    "\n",
    "# 6. Create the output layer and add it to the model object:\n",
    "\n",
    "# Create input layer:\n",
    "output_layer =Dense(units=1, activation='sigmoid')\n",
    "\n",
    "# Add output_layer to the model object:\n",
    "nn_model.add(output_layer)\n",
    "\n",
    "\n",
    "# Print summary of neural network model structure\n",
    "nn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Define the Optimization Function\n",
    "\n",
    "<b>Task:</b> In the code cell below, create a stochastic gradient descent optimizer using  `keras.optimizers.SGD()`. Specify a learning rate of 0.1 using the `learning_rate` parameter. Assign the result to the variable`sgd_optimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "sgd_optimizer = SGD(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define the Loss Function\n",
    "\n",
    "<b>Task:</b> In the code cell below, create a binary cross entropy loss function using `keras.losses.BinaryCrossentropy()`. Use  the parameter `from_logits=False`. Assign the result to the variable  `loss_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "loss_fn = BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Compile the Model\n",
    "\n",
    "<b>Task:</b> In the code cell below, package the network architecture with the optimizer and the loss function using the `compile()` method. \n",
    "\n",
    "\n",
    "You will specify the optimizer, loss function and accuracy evaluation metric. Call the `nn_model.compile()` method with the following arguments:\n",
    "* Use the `optimizer` parameter and assign it your optimizer variable:`optimizer=sgd_optimizer`\n",
    "* Use the `loss` parameter and assign it your loss function variable: `loss=loss_fn`\n",
    "* Use the `metrics` parameter and assign it the `accuracy` evaluation metric: `metrics=['accuracy']`\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(optimizer=sgd_optimizer, loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Fit the Model on the Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define our own callback class to output information from our model while it is training. Make sure you execute the code cell below so that it can be used in subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgBarLoggerNEpochs(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, num_epochs: int, every_n: int = 50):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.every_n = every_n\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.every_n == 0:\n",
    "            s = 'Epoch [{}/ {}]'.format(epoch + 1, self.num_epochs)\n",
    "            logs_s = ['{}: {:.4f}'.format(k.capitalize(), v)\n",
    "                      for k, v in logs.items()]\n",
    "            s_list = [s] + logs_s\n",
    "            print(', '.join(s_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> In the code cell below, fit the neural network model to the vectorized training data.\n",
    "\n",
    "1. Call `nn_model.fit()` with the training data `X_train_tfidf` and `y_train` as arguments. Note that `X_train_tfidf` is currently of type sparse matrix. The Keras `fit()` method requires that input data be of specific types. One type that is allowed is a NumPy array. Convert `X_train_tfidf` to a NumPy array using the `toarray()` method.\n",
    "\n",
    "\n",
    "2. In addition, specify the following parameters:\n",
    "\n",
    "    * Use the `epochs` parameter and assign it the variable to `epochs`: `epochs=num_epochs`\n",
    "    * Use the `verbose` parameter and assign it the value of  0: `verbose=0`\n",
    "    * Use the `callbacks` parameter and assign it a list containing our logger function: \n",
    "    `callbacks=[ProgBarLoggerNEpochs(num_epochs_M, every_n=50)]`  \n",
    "    * We will use a portion of our training data to serve as validation data. Use the  `validation_split` parameter and assign it the value `0.2`\n",
    "    \n",
    "    \n",
    "    \n",
    "3. Save the results to the variable `history`. \n",
    "\n",
    "<b>Note</b>: This may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "X_train_tfidf_np = X_train_tfidf.toarray()\n",
    "\n",
    "t0 = time.time() # start time\n",
    "\n",
    "num_epochs = 30 #epochs\n",
    "callbacks_list = [ProgBarLoggerNEpochs(num_epochs, every_n=50)]\n",
    "\n",
    "history = nn_model.fit(X_train_tfidf_np, y_train, epochs=num_epochs, verbose=0, callbacks=callbacks_list, validation_split=0.2)\n",
    "\n",
    "\n",
    "t1 = time.time() # stop time\n",
    "\n",
    "print('Elapsed time: %.2fs' % (t1-t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Model's Performance Over Time\n",
    "\n",
    "The code above outputs both the training loss and accuracy and the validation loss and accuracy. Let us visualize the model's performance over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.plot(range(1, num_epochs + 1), history.history['loss'], label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_loss'], label='Validation Loss')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.plot(range(1, num_epochs + 1), history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs + 1), history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Improve the Model and Evaluate the Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just evaluated our model's performance on the training and validation data. Let's now evaluate its performance on our test data and compare the results.\n",
    "\n",
    "Keras makes the process of evaluating our model very easy. Recall that when we compiled the model we specified the metric we wanted to use to evaluate the model: accuracy. The Keras method `evaluate()` will return the loss and accuracy score of our model on our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> In the code cell below, call `nn_model.evaluate()` with `X_test_tfidf` and `y_test` as arguments. You must convert `X_test_tfidf` to a NumPy array using the `toarray()` method. \n",
    "\n",
    "Note: The `evaluate()` method returns a list containing two values. The first value is the loss and the second value is the accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tfidf_np = X_test_tfidf.toarray()\n",
    "loss, accuracy = nn_model.evaluate(X_test_tfidf_np, y_test)\n",
    "\n",
    "print('Loss: ', str(loss) , 'Accuracy: ', str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prevent Overfitting and Improve Model's Performance\n",
    "\n",
    "Neural networks can be prone to overfitting. Notice that the training accuracy is 100% but the test accuracy is around 82%. This indicates that our model is overfitting; it will not perform as well to new, previously unseen data as it did during training. We want to have an accurate idea of how well our model will generalize. Our goal is to have our training and testing accuracy scores be as close as possible.\n",
    "\n",
    "While there are different techniques that can be used to prevent overfitting, for the purpose of this exercise, we will focus on two methods:\n",
    "\n",
    "1. Changing the number of epochs. Too many epochs can lead to overfitting of the training dataset, whereas too few epochs may result in underfitting.\n",
    "\n",
    "2. Adding dropout regularization. During training, the nodes of a particular layer may always become influenced only by the output of a particular node in the previous layer, causing overfitting. Dropout regularization is a technique that randomly drops a number of nodes in a neural network during training as a way to adding randomization and prevent nodes from becoming dependent on one another. Adding dropout regularization can reduce overfitting and also improve the performance of the model. \n",
    "\n",
    "<b>Task:</b> \n",
    "\n",
    "1. Tweak the variable `num_epochs` above and restart and rerun all of the cells above. Evaluate the performance of the model on the training data and the test data.\n",
    "\n",
    "2. Add Keras `Dropout` layers after one or all hidden layers. Add the following line of code after you add a hidden layer to your model object:  `nn_model.add(keras.layers.Dropout(.25))`. The parameter `.25` is the fraction of the nodes to drop. You can experiment with this value as well. Restart and rerun all of the cells above. Evaluate the performance of the model on the training data and the test data.\n",
    "\n",
    "\n",
    "<b>Analysis:</b> \n",
    "In the cell below, specify the different approaches you used to reduce overfitting and summarize which configuration led to the best generalization performance.\n",
    "\n",
    "Did changing the number of epochs prevent overfitting? Which value of `num_epochs` yielded the closest training and testing accuracy score? Recall that too few epochs can lead to underfitting (both poor training and test performance). Which value of `num_epochs` resulted in the best accuracy score when evaluating the test data?\n",
    "\n",
    "Did adding dropout layers prevent overfitting? How so? Did it also improve the accuracy score when evaluating the test data? How many dropout layers did you add and which fraction of nodes did you drop? \n",
    "\n",
    "Record your findings in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MY RESPONSE: \n",
    "The number of epochs determines how many times the model is trained on the entire dataset. I experimented with different values for num_epochs and found that increasing the number of epochs did not prevent overfitting. It led to a gap between the training and testing accuracy scores. The value of num_epochs that yielded the closest training and testing accuracy scores was 15 epochs. \n",
    "\n",
    "Dropout layers randomly drop out a fraction of nodes during training, preventing the model from relying too much on specific nodes and increasing its generalization capability. I added dropout layers after the first two hidden layers with a dropout fraction of 0.25. This introduced some randomness during training, preventing the model from memorizing the training data and improving its ability to generalize to new, unseen data. The addition of Dropout layers not only effectively prevented overfitting but also resulted in improved accuracy when evaluating the test data. The model achieved better generalization performance, as evidenced by its higher accuracy on the test set.\n",
    "\n",
    "In conclusion, the combination of adding Dropout layers and reducing the number of epochs to an optimal value (15 epochs) led to the best generalization performance and highest accuracy when evaluating the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our best performing model that can generalize to new, previously unseen data, let us make predictions using our test data.\n",
    "\n",
    "In the cell below, we will make a prediction on our test set using the `predict()` method, receive a probability between 0.0 and 1.0, and then apply a threshold to obtain the the predicted class for each example. We will use a threshold of 0.5.\n",
    "\n",
    "For the first 10 examples, we will output their probabilities and the corresponding classes. Examine the output to see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_predictions = nn_model.predict(X_test_tfidf.toarray())\n",
    "\n",
    "print(\"Predictions for the first 10 examples:\")\n",
    "print(\"Probability\\t\\t\\tClass\")\n",
    "for i in range(0,10):\n",
    "    if probability_predictions[i] >= .5:\n",
    "        class_pred = \"Good Review\"\n",
    "    else:\n",
    "        class_pred = \"Bad Review\"\n",
    "    print(str(probability_predictions[i]) + \"\\t\\t\\t\" + str(class_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check two book reviews and see if our model properly predicted whether the reviews are good or bad reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Review #1:\\n')\n",
    "print(X_test.to_numpy()[56])\n",
    "\n",
    "goodReview = True if probability_predictions[56] >= .5 else False\n",
    "    \n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(goodReview))\n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[56]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Review #2:\\n')\n",
    "print(X_test.to_numpy()[24])\n",
    "\n",
    "goodReview = True if probability_predictions[24] >= .5 else False\n",
    "\n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(goodReview)) \n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[24]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive (Ungraded):\n",
    "\n",
    "Experiment with the vectorizer and neural network implementation above and compare your results every time you train the network. Pay attention to the time it takes to train the network, and the resulting loss and accuracy on both the training and test data. \n",
    "\n",
    "Below are some ideas for things you can try:\n",
    "\n",
    "* Adjust the learning rate.\n",
    "* Add more hidden layers and/or experiment with different values for the `unit` parameter in the hidden layers to change the number of nodes in the hidden layers.\n",
    "* Fit your vectorizer using different document frequency values and a different n-gram ranges. When creating a `TfidfVectorizer` object, use the parameter `min_df` to specify the minimum 'document frequency' and use `ngram_range=(1,2)` to change the default n-gram range of `(1,1)`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
